input: 6 images 3x900x1600

simple_test(img_metas,img,rescale):

	1) img_feats = self.EXTRACT_FEATS(img=img, img_metas=img_metas) : extract feature with ResNet+FPN -> 6x256x29x50

	2) outs = self.PTS_BBOX_HEAD(img_feats, img_metas) : features go into Detr3d Head 

		# create embedding of 900 queries of size 512, and extract the weights

		2.1) hs, init_reference, inter_references = self.TRANSFORMER(mlvl_feats,query_embeds,reg_branches=self.reg_branches,img_metas=img_metas):

			# split query embedding to query_prior and query_pos_encoding

			# project query_pos_enc to 3d space, sigmoid it and save in ref_points_prior (=ref_points) ### Formula 4 ###

			# extract img h,w and padding from metas

			# create img position mask 

			# positional encoding of img position mask (key) by interpolating with img size

			2.1.1) inter_queries, inter_ref_points = self.DECODER(query=query_prior,key=mlvl_img_feats,value=mlvl_img_feats,query_pos=query_pos_encoding,key_pos=pos_encodings,ref_points=ref_points,reg_branches=reg_branches,**kwargs): run the 6x decoder layers

				# BaseTransformerLayer manages the 6 decoder layers. Each layer contains self-attn, cross-attn, norm and ffn
				
				FOR EACH DECODER LAYER:

					# update query by applying SelfAttention+norm+CrossAttention+norm+ffn+norm
					2.1.1.1) query = layer(*args,query,value=value[0],key=key[0],key_pos=key_pos[0],reference_points=ref_points_input,**kwargs)

							# operation_order=("self_attn","norm", "cross_attn", "norm", "ffn", "norm")

						    # MultiheadAttention.forward: self-attention (query=key) to update the queries by interacting with themselves

						    # Applies norm (LayerNorm) to the queries

						    # QueryValueProjectCrossAttention.forward: cross-attention between queries and feature keys

						    	# Extract BS, CAMS and QUERIES dimensions from shapes

						    	# Extract cam_T_lidar from img_metas, and calculate lidar_T_cam as inverse of it

						    	# Save reference_points in reference_points_orig, it will be used after attention

						    	# Update reference_points with pc_range

						    	# Create reference_points_homogenous = torch.cat(reference_points, ones_query)

						    	# Create query_per_cam tensor [CAMS, QUERIES, BS, self.embed_dims]

						    	# Create values_global tensor

						    	# UPDATE LATENT KEY WITH GEOMETRICAL POSITIONAL ENCODING: feats_with_dir = value + self.query_loc2latent(key_pos) ### Formula 3 ###

						    	# COMPUTE DEPTH ESTIMATE FOR EACH LATENT KEY : value_3d_cam = self.value2depth(feats_with_dir) ### Formula 8 ###

						    	# SCALE THE DIRECTION VECTOR KEY_POS WITH THE DEPTH ESTIMATE: value_3d_cam = value_3d_cam * key_pos ### Formula 9 ###

						    	FOR EACH CAM:

						    		# PROJECT QUERY CENTER TO CAMERA: reference_points_cam = torch.bmm(reference_points_homogenous, cam_T_lidar_tensor[cam_idx])[..., 0:3] ### Formula 5 ###

						    		# normalize it

						    		# query_per_cam[cam_idx] = query + query_pos + self.query_loc2latent(reference_points_cam) ### Formula 6 ###

						    		
						     
						    # Norm+FFN+Norm


					# obb calculated with regression branch
					obb = reg_branches[layer_num](query)

					# query object center are updated [formula 1]
					ref_points = sigmoid(sigm_inv(ref_points) + obb) 

					# append query and ref_points to inter_queries and inter_ref_points list

		    	return inter_queries, inter_ref_points

		
		 	return inter_queries (hs), ref_points_prior (init_referece), inter_ref_points (inter_references)

		
		for each decoder layer:

			# take reference as ref_points_prior if it is the first layer, otherwise take the corrisponding inter_reference; inverse sigmoid it

			# infer outpus class from query hs with corrisponding cls_branch layer 
			outputs_class = self.cls_branches[lvl](hs[lvl])

			# infer bbox coord from query hs with corrisponding reg_branch layer
			tmp = self.reg_branches[lvl](hs[lvl])

			# update bbox coord by offsetting with reference, and do the sigmoid.

			# append the output class and the updated bbox coord to lists

        outputs_classes = torch.stack(outputs_classes)
        outputs_coords = torch.stack(outputs_coords)
        outs = {
            'all_cls_scores': outputs_classes, [6,1,900,10]
            'all_bbox_preds': outputs_coords, [6,1,900,10]
            'enc_cls_scores': None,
            'enc_bbox_preds': None, 
        }
 

	3) bbox_list = self.PTS_BBOX_HEAD.GET_BBOXES(outs, img_metas) : apply NMS-free bbox coder

	4) bbox_results = [bbox3d2result(bboxes, scores, labels) for bboxes, scores, labels in bbox_list] 

	return bbox_results 
	
Creates list of dict, with bbox and labels, and returns

return bbox_list -> {"pts_bbox" : {"boxes_3d" = LidarInstance3dBoxes.., "scores_3d" = [300], "labels_3d" = [300]}}


