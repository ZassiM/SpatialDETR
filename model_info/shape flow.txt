
outputs = single_gpu_test(model, data_loader, args.show, args.show_dir)

    result = model(return_loss=False, rescale=True, **data) 
        Detr3D

        forward->forward_test->simple_test
        simple_test(img_metas,img,rescale):

            1) Backbone (ResNet) + mask + Neck (FPN)
                -> images BxNxCxHxW
            img_feats = self.extract_feat(img=img, img_metas=img_metas)
                -> list of features Bx(BN/B)xCxHxW

            2) Applies head 
            bbox_pts = self.simple_test_pts (img_feats, img_metas, rescale=rescale):
                
                2.1) Uses Detr3DHead  *******
                 -> mlvl_feas [1, 6, 256, 29, 50]
                outs = self.pts_bbox_head(img_feats, img_metas)
                -> all_cls_scores [6, 1, 900, 10], all_bbox_preds [6, 1, 900, 10]

                2.2) Generate bboxes from bbox head predictions with NMSFreeCoder
                bbox_list = self.pts_bbox_head.get_bboxes(outs, img_metas, rescale=rescale)
                -> {"LiDARInstance3DBoxes" = bev,center,height,corners..., "0" = [300], "1" = [300]}

                2.3) Convert detection results to a list of numpy arrays.
                bbox_results = [bbox3d2result(bboxes, scores, labels) for bboxes, scores, labels in bbox_list]
                ->  {"boxes_3d" = LidarInstance3dBoxes.., "scores_3d" = [300], "labels_3d" = [300]}

                return bbox_results

            3) Creates list of dict, with bbox and labels, and returns
            bbox_list = [dict() for i in range(len(img_metas))]
            for result_dict, pts_bbox in zip(bbox_list, bbox_pts):
                result_dict['pts_bbox'] = pts_bbox

            -> {"pts_bbox" : {"boxes_3d" = LidarInstance3dBoxes.., "scores_3d" = [300], "labels_3d" = [300]}}

            return bbox_list


DETAILED HEAD PART (2.1)
    
-> mlvl_feats([1, 6, 256, 29, 50]), img_metas = "filename", "ori_shape", "img_shape" ...
    outs = self.pts_bbox_head(img_feats = mlvl_feats, img_metas)

        Detr3DHead.forward(mlvl_feats, img_metas):

            ->self.query_embedding [Embedding(900, 512)]
                query_embeds = self.query_embedding.weight
            ->query_embeds [900,512]
            
                2.1.1
                hs, init_reference, inter_references = self.transformer(
                    mlvl_feats,
                    query_embeds,
                    reg_branches=self.reg_branches if self.with_box_refine else None,  # noqa:E501
                    img_metas=img_metas,
                )
            -> hs [6, 900, 1, 256], init_reference [1, 900, 3], inter_references [6, 1, 900, 3]  # 6 = num_tr_layers

                hs = hs.permute(0, 2, 1, 3)
            -> hs [6, 1, 900, 256]

                outputs_classes = []
                outputs_coords = []

                for lvl in range(hs.shape[0]):
                    if lvl == 0:
                        reference = init_reference
                    else:
                        reference = inter_references[lvl - 1]

                    reference = inverse_sigmoid(reference)
                    outputs_class = self.cls_branches[lvl](hs[lvl])
                    tmp = self.reg_branches[lvl](hs[lvl])

                    # TODO: check the shape of reference
                    assert reference.shape[-1] == 3
                    tmp[..., 0:2] += reference[..., 0:2]
                    tmp[..., 0:2] = tmp[..., 0:2].sigmoid()
                    tmp[..., 4:5] += reference[..., 2:3]
                    tmp[..., 4:5] = tmp[..., 4:5].sigmoid()
                    tmp[..., 0:1] = (tmp[..., 0:1] * (self.pc_range[3] - self.pc_range[0]) + self.pc_range[0])
                    tmp[..., 1:2] = (tmp[..., 1:2] * (self.pc_range[4] - self.pc_range[1]) + self.pc_range[1])
                    tmp[..., 4:5] = (tmp[..., 4:5] * (self.pc_range[5] - self.pc_range[2]) + self.pc_range[2])

                    # TODO: check if using sigmoid
                    outputs_coord = tmp
                    outputs_classes.append(outputs_class)
                    outputs_coords.append(outputs_coord)

                outputs_classes = torch.stack(outputs_classes)
                outputs_coords = torch.stack(outputs_coords)
                outs = {
                    'all_cls_scores': outputs_classes, [6, 1, 900, 10]
                    'all_bbox_preds': outputs_coords, [6, 1, 900, 10]
                    'enc_cls_scores': None,
                    'enc_bbox_preds': None, 
                }
                return outs

DETAILED TRANSFORMER PART (2.1.1)

    hs, init_reference, inter_references = self.transformer(
        mlvl_feats,
        query_embeds,
        reg_branches=self.reg_branches if self.with_box_refine else None,  # noqa:E501
        img_metas=img_metas,
    )
    -> mlvl_img_feats [1, 6, 256, 29, 50], query_embed [900,512], reg_branches = 5xSequential(Linear,Relu,Linear,Relu,Linear->out 10)
        SpatialDETRTransformer.forward(mlvl_img_feats, query_embed, reg_branches): 


                BS = mlvl_img_feats[0].size(0)
            -> BS = 1

                query_pos_encoding, query_prior = torch.split(query_embed, self.embed_dims, dim=1)
            -> query_pos_encoding [900,256], query_prior [900,256]

                query_pos_encoding = query_pos_encoding.unsqueeze(0).expand(BS, -1, -1)
                query_prior = query_prior.unsqueeze(0).expand(BS, -1, -1)
            -> query_pos_encoding [1,900,256], query_prior [1,900,256]

                ref_points = self.latent2ref(query_pos_encoding)
            -> ref_points [1,900,3]

                # convert to sigmoid space
                ref_points = ref_points.sigmoid()
                ref_points_prior = ref_points

                query_prior = query_prior.permute(1, 0, 2) -> [900,1,256]
                query_pos_encoding = query_pos_encoding.permute(1, 0, 2) -> [900,1,256]

                # Positional encoding
                input_img_h, input_img_w = kwargs["img_metas"][0]["ori_shape"][0:2] -> 900, 1600
                padded_h, padded_w, _ = kwargs["img_metas"][0]["pad_shape"][0] -> 928, 1600

                img_position_mask = torch.ones((BS, padded_h, padded_w), device=mlvl_img_feats[0].device, requires_grad=False)
                img_position_mask[:, :input_img_h, :input_img_w] = 0
            -> [1, 928, 1600]

                pos_encodings = []
                # build pos encoding for each feature lvl:
                for lvl in range(len(mlvl_img_feats)):

                    feature_height = mlvl_img_feats[lvl].shape[-2]
                    feature_width = mlvl_img_feats[lvl].shape[-1]

                    # interpolate masks to have the same spatial shape with feats per cam
                    # squeeze is needed since interpolate expects a channel dimension
                    img_position_mask_feature = (
                        F.interpolate(img_position_mask.unsqueeze(
                            1), size=(feature_height, feature_width))
                        .to(torch.bool)
                        .squeeze(1)
                    )

                    # cams x bs x dim x h x w
                    pos_encoding = self.pos_encoding(
                        img_position_mask_feature, kwargs["img_metas"])

                    # permute:
                    # cams x bs x dim x h x w -> bs x c x d x h x w
                    pos_encoding = pos_encoding.permute(1, 0, 2, 3, 4)

                    pos_encodings.append(pos_encoding)

                2.1.1.1
                inter_queries, inter_ref_points = self.decoder(
                    query=query_prior,
                    key=mlvl_img_feats,  # will be set to feats
                    value=mlvl_img_feats,
                    query_pos=query_pos_encoding,
                    key_pos=pos_encodings,
                    ref_points=ref_points,
                    reg_branches=reg_branches,
                    **kwargs
                )

                return inter_queries, ref_points_prior, inter_ref_points


DECODER DETAILED PART (2.1.1.1)


TORCH SHAPE FLOW

TEST.PY
outputs = single_gpu_test(model, data_loader, args.show, args.show_dir):

API/TEST.PY

    for i, data in enumerate(data_loader):

    data = {"img_metas": 
                {'filename': [6 CAMERAS SAMPLES, ex: ['./data/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151603512404.jpg']], ''ori_shape': (900, 1600, 3, 6)', 'img_shape': (900, 1600, 3, 6)x6, 'lidar2img' : 6x[4tensor], 'pad_shape': [(...), (...), (...), (...), (...), (...)], 'scale_factor': 1.0, 'flip': False, 'pcd_horizontal_flip': False, 'pcd_vertical_flip': False, 'box_mode_3d': <Box3DMode.LIDAR: 0>, 'box_type_3d': <class 'mmdet3d.core.bbox.structures.lidar_box3d.LiDARInstance3DBoxes'>, 'img_norm_cfg': {'mean': array([103.53 , 116.28 , 123.675], dtype=float32), 'std': array([1., 1., 1.], dtype=float32), 'to_rgb': False}, 'sample_idx': '3e8750f331d7499e9b5123e9eb70f2e2', 'pcd_scale_factor': 1.0, 'pts_filename': ../LIDAR_TOP, 'cam_intrinsic': 4x4, 'lidar2cam': 4x4, 'input_shape': 928x1600}
            
            "img": 
                DataContainer with tensors 1x6x3x928x1600}

        result = model(return_loss=False, rescale=True, **data)


    result = {'pts_bbox': {

             boxes_3d': LiDARInstance3DBoxes(
                    tensor 300(NMS_MAX)x9

            'scores_3d': tensor 300
                tensor([0.9440, 0.9122, 0.8962, 0.8811, 0.8685, 0.7916, 0.7351, 0.7337, 0.6348,
                0.5338, 0.5105, 0.4513, 0.4350, 0.4303, 0.4200, 0.4099, 0.4086, 0.4004,
                0.4001, 0.3897, 0.3812, 0.3793, 0.3755, 0.3641, 0.3609, 0.3591, 0.3543,
                0.3540, 0.3422, 0.3414, 0.3370, 0.3330, 0.3266, 0.3261, 0.3258, 0.3257,
                0.3248, 0.3245, 0.3203, 0.3186, 0.3182, 0.3182, 0.3152, 0.3122, 0.3102,
                0.3099, 0.3093, 0.3080, 0.3077, 0.3062, 0.3015, 0.3008, 0.3001, 0.2993,
                0.2957, 0.2956, 0.2950, 0.2937, 0.2936, 0.2924, 0.2898, 0.2894, 0.2891,
                0.2882, 0.2779, 0.2752, 0.2738, 0.2707, 0.2704, 0.2699, 0.2671, 0.2668,
                0.2666, 0.2621, 0.2621, 0.2612, 0.2601, 0.2598, 0.2525, 0.2515, 0.2498,
                0.2475, 0.2469, 0.2454, 0.2435, 0.2403, 0.2388, 0.2387, 0.2383, 0.2356,
                0.2342, 0.2321, 0.2318, 0.2304, 0.2285, 0.2280, 0.2267, 0.2258, 0.2222,
                0.2215, 0.2195, 0.2159, 0.2134, 0.2119, 0.2117, 0.2095, 0.2091, 0.2076,
                0.2068, 0.2029, 0.2008, 0.1988, 0.1957, 0.1913, 0.1871, 0.1851, 0.1848,
                0.1819, 0.1804, 0.1803, 0.1800, 0.1792, 0.1792, 0.1783, 0.1770, 0.1765,
                0.1763, 0.1752, 0.1738, 0.1736, 0.1728, 0.1719, 0.1704, 0.1703, 0.1696,
                0.1679, 0.1669, 0.1653, 0.1629, 0.1622, 0.1603, 0.1601, 0.1598, 0.1593,
                0.1589, 0.1588, 0.1585, 0.1571, 0.1567, 0.1535, 0.1523, 0.1509, 0.1509,
                0.1487, 0.1483, 0.1482, 0.1480, 0.1478, 0.1473, 0.1467, 0.1466, 0.1465,
                0.1425, 0.1416, 0.1378, 0.1341, 0.1336, 0.1335, 0.1334, 0.1329, 0.1319,
                0.1315, 0.1301, 0.1299, 0.1296, 0.1267, 0.1252, 0.1244, 0.1221, 0.1198,
                0.1192, 0.1190, 0.1176, 0.1159, 0.1126, 0.1121, 0.1107, 0.1105, 0.1094,
                0.1091, 0.1084, 0.1083, 0.1075, 0.1060, 0.1058, 0.1029, 0.1023, 0.1014,
                0.1004, 0.0987, 0.0980, 0.0956, 0.0948, 0.0948, 0.0939, 0.0924, 0.0904,
                0.0885, 0.0873, 0.0861, 0.0817, 0.0794, 0.0791, 0.0777, 0.0769, 0.0755,
                0.0743, 0.0742, 0.0741, 0.0740, 0.0739, 0.0723, 0.0698, 0.0693, 0.0692,
                0.0660, 0.0653, 0.0652, 0.0651, 0.0649, 0.0643, 0.0641, 0.0624, 0.0618,
                0.0607, 0.0607, 0.0593, 0.0589, 0.0587, 0.0587, 0.0577, 0.0575, 0.0574,
                0.0568, 0.0563, 0.0554, 0.0553, 0.0550, 0.0549, 0.0548, 0.0543, 0.0540,
                0.0534, 0.0534, 0.0533, 0.0524, 0.0521, 0.0519, 0.0508, 0.0501, 0.0479,
                0.0479, 0.0469, 0.0468, 0.0466, 0.0466, 0.0459, 0.0458, 0.0456, 0.0453,
                0.0453, 0.0448, 0.0448, 0.0447, 0.0447, 0.0446, 0.0443, 0.0432, 0.0424,
                0.0423, 0.0421, 0.0420, 0.0410, 0.0408, 0.0407, 0.0407, 0.0405, 0.0401,
                0.0401, 0.0397, 0.0397, 0.0392, 0.0391, 0.0389, 0.0386, 0.0381, 0.0381,
                0.0380, 0.0379, 0.0376]), 

        'labels_3d': tensor 300
            tensor([0, 8, 0, 8, 8, 8, 8, 8, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 8, 8, 8, 8,
            8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 8, 8, 8, 8, 8, 8, 8,
            8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
            8, 8, 8, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 8, 8, 8, 8, 8, 8, 8,
            8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
            8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 8, 8, 0, 8, 8, 8, 8, 8, 0, 8, 8, 8, 8,
            8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
            8, 8, 0, 8, 8, 8, 8, 0, 8, 8, 8, 8, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
            8, 0, 8, 8, 0, 8, 8, 8, 8, 0, 8, 8, 8, 8, 0, 0, 0, 8, 8, 8, 0, 8, 8, 8,
            9, 8, 8, 8, 8, 8, 0, 7, 8, 8, 8, 8, 9, 8, 8, 8, 8, 8, 8, 0, 8, 8, 8, 8,
            8, 8, 0, 7, 8, 8, 0, 0, 7, 8, 8, 0, 7, 8, 0, 8, 7, 8, 1, 0, 8, 8, 7, 0,
            8, 8, 0, 8, 6, 8, 8, 9, 0, 8, 8, 0, 8, 1, 8, 8, 8, 8, 6, 8, 0, 7, 9, 8,
            6, 0, 8, 8, 8, 7, 1, 7, 7, 8, 0, 8])}

            }


